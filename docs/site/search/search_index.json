{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Leeroo Orchestrator","text":"<p>For more details refer the paper Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration.</p>"},{"location":"#project-layout","title":"Project layout","text":"<ul> <li><code>orchestrator.py</code> - The Orchestrator serves as the central wrapper object, facilitating seamless coordination among expert models.</li> <li><code>server_manager.py</code> - Manages the servers implemented in the <code>./servers/</code> directory.</li> <li><code>test_orchestrator.py</code> - Provides an example of running the orchestrator for testing purposes.</li> <li><code>abstract_classes.py</code> - Contains base classes for servers, serving as foundational structures for server implementations.</li> <li><code>./configs/*</code> - Holds configuration files used by the orchestrator for various settings.</li> <li><code>./servers/*</code> - Houses all server implementations, each dedicated to serving specific serving strategy/machine i.e Ec2 vllm/sagemaker/... etc.</li> <li><code>./utils/*</code> - Includes utility functions used by servers to enhance functionality.</li> <li><code>.env</code> - Specifies the required environment variables for proper system operation.</li> </ul> <pre><code>\nAWS_SAGEMAKER_ROLE_NAME = \"*****\"   # Role is used for sagemaker connections.\nSECURITY_GROUP_ID       = \"****\"    # SecurityGroupId is used to create an instance. make sure to have inference port open for this group.\nAWS_ACCESS_KEY_ID       = \"*****\"   # \nAWS_SECRET_ACCESS_KEY   = \"*****\"   #\nHUGGING_FACE_HUB_TOKEN  = \"*****\"   # Required for authenticating with HF for downloading checkopints\nOPENAI_ORGANIZATION     = \"*****\"\nOPENAI_API_KEY          = \"*****\"   # If openai models are used as experts\n</code></pre>"},{"location":"#run-orchestrator-server","title":"Run Orchestrator server","text":"<pre><code>import json\nimport time\nfrom app.orchestrator import Orchestrator\n\n\nconfig = json.load(open(\"app/configs/demo_orch_sagemaker_mix.json\", \"r\"))\n\n# init\nleeroo_orchestrator = Orchestrator(config)\n\n# boot the machines\nleeroo_orchestrator.load_orchestrator_server()\nleeroo_orchestrator.load_experts_server()\n\n# start the inference endpoints\nleeroo_orchestrator.start_inference_endpoints(max_wait_time=120)\n\n\n# Wait until all endpoints are up\nstatus = False\nwhile not status:\n    print(\"Checking server status...\")\n    status = leeroo_orchestrator.check_servers_state()\n    if status:\n        print(\"Servers are running...\")\n        break\n    time.sleep(30)\n\n# Test get_response for all the servers\nfor expert in leeroo_orchestrator.experts.values():\n    print(expert.model_id)\n    print(expert.get_response(\"hello\"))\n\n# Test get_response for complete pipeline\nresponse = leeroo_orchestrator.get_response(\"What is the capital of India?\")\nprint(response)\n\n# turn off the machines\nleeroo_orchestrator.orchestrator.stop_server()\nfor expert_id, expert in leeroo_orchestrator.experts.items():\n    res = expert.stop_server()\n    print(res)\n\nprint(\"done!\")\n</code></pre>"},{"location":"Orchestrator/","title":"Orchestrator","text":"<p>The Orchestrator functions as the central wrapper object, facilitating the seamless deployment of underlying models. Each individual model can be hosted on a dedicated server for efficient and scalable service. Furthermore, multiple models can be hosted on a shared server. Refer to the configurations in the <code>configs/demo*</code> directory for illustrative examples.</p> Source code in <code>app/orchestrator.py</code> <pre><code>class Orchestrator:\n    \"\"\"The Orchestrator functions as the central wrapper object, facilitating the seamless deployment of underlying models. Each individual model can be hosted on a dedicated server for efficient and scalable service.\n    Furthermore, multiple models can be hosted on a shared server. Refer to the configurations in the `configs/demo*` directory for illustrative examples.\n    \"\"\"\n    def __init__(self, config, verbose=True):\n        \"\"\"init\n\n        Args:\n            config (json): The configuration file contains all the necessary information required to initialize a server. Refer to the examples in the `app/config` directory for guidance.\n            verbose (bool, optional): Defaults to True.\n        \"\"\"\n        self.config = config\n        self.verbose = verbose\n        self.orchestrator = get_server(config['orchestrator'], \"orchestrator\")\n        self.experts = {expert_config['expert_id']: get_server(expert_config) \\\n                        for expert_config in self.config['experts']}\n        self.config['logs'] = []\n\n    def load_orchestrator_server(self):\n        \"\"\"Loads the Orchestrator Server. Activates the dedicated machine using specifications from the configuration for the orchestrator server.\n        \"\"\"\n        if self.orchestrator.config.get('s3_download_path'):\n            raise NotImplementedError\n        instance_state = self.orchestrator.start_server()\n        self.config['logs'].append('starting orchestrator')\n\n    def load_experts_server(self):\n        \"\"\"\n        Load Servers for Each Expert\n        Activates dedicated machines using specifications from the configuration for each of the Experts' servers.\n        \"\"\"\n        for expert_id,expert in self.experts.items():\n            instance_state = expert.start_server()\n            self.config['logs'].append(f'starting expert {expert_id}')\n\n    def start_inference_endpoints(self, max_wait_time=120):\n        \"\"\"Start Inference Endpoint for Orchestrator and All Experts.\n        The inference endpoints for each model can be served using VLLM or AWS Sagemaker.\n        Note: For closed-source endpoints like GPT-4, `start_inference_endpoint` is non-functional.\n\n        Args:\n            max_wait_time (int, optional): Defaults to 120.\n        \"\"\"\n        self.orchestrator.start_inference_endpoint(max_wait_time)\n        for expert in self.experts.values():\n            expert.start_inference_endpoint(max_wait_time)\n\n    def check_servers_state(self):\n        \"\"\"Perform a ping test on each server to confirm their operational status.\n\n        Returns:\n            status (bool): True/False if the the inference endpoint is running.\n        \"\"\"\n        status = []\n        state = self.orchestrator.check_servers_state()\n        print(f\"Model id {self.orchestrator.instance_name} : {state}\")\n        status.append(state[0])\n        for expert in self.experts.values():\n            state = expert.check_servers_state()\n            print(f\"Model id {expert.instance_name} : {state}\")\n            status.append(state[0])\n        return False not in status\n\n    def get_response(self,query):\n        \"\"\"Get the Response for input query.\n        The input query undergoes orchestration to determine the most suitable expert model. Subsequently, the query is processed through the chosen expert model, and the resulting response is returned.   \n\n        Args:\n            query (str): Input Query.\n\n        Returns:\n            response (str): Response.\n        \"\"\"\n        expert_id = self.get_orchestrator_response(query)\n        response = self.get_expert_response(query,expert_id)\n        return response\n\n    def get_orchestrator_response(self,query):\n        expert_id = self.orchestrator.get_response(query)\n        print(f\"Selecting Expert Id {expert_id} for current query\")\n        return expert_id\n\n    def get_expert_response(self,query,expert_id):\n        expert = self.experts.get(expert_id)\n        response = expert.get_response(query)\n        return response\n</code></pre>"},{"location":"Orchestrator/#app.orchestrator.Orchestrator.__init__","title":"<code>__init__(config, verbose=True)</code>","text":"<p>init</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>json</code> <p>The configuration file contains all the necessary information required to initialize a server. Refer to the examples in the <code>app/config</code> directory for guidance.</p> required <code>verbose</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> Source code in <code>app/orchestrator.py</code> <pre><code>def __init__(self, config, verbose=True):\n    \"\"\"init\n\n    Args:\n        config (json): The configuration file contains all the necessary information required to initialize a server. Refer to the examples in the `app/config` directory for guidance.\n        verbose (bool, optional): Defaults to True.\n    \"\"\"\n    self.config = config\n    self.verbose = verbose\n    self.orchestrator = get_server(config['orchestrator'], \"orchestrator\")\n    self.experts = {expert_config['expert_id']: get_server(expert_config) \\\n                    for expert_config in self.config['experts']}\n    self.config['logs'] = []\n</code></pre>"},{"location":"Orchestrator/#app.orchestrator.Orchestrator.check_servers_state","title":"<code>check_servers_state()</code>","text":"<p>Perform a ping test on each server to confirm their operational status.</p> <p>Returns:</p> Name Type Description <code>status</code> <code>bool</code> <p>True/False if the the inference endpoint is running.</p> Source code in <code>app/orchestrator.py</code> <pre><code>def check_servers_state(self):\n    \"\"\"Perform a ping test on each server to confirm their operational status.\n\n    Returns:\n        status (bool): True/False if the the inference endpoint is running.\n    \"\"\"\n    status = []\n    state = self.orchestrator.check_servers_state()\n    print(f\"Model id {self.orchestrator.instance_name} : {state}\")\n    status.append(state[0])\n    for expert in self.experts.values():\n        state = expert.check_servers_state()\n        print(f\"Model id {expert.instance_name} : {state}\")\n        status.append(state[0])\n    return False not in status\n</code></pre>"},{"location":"Orchestrator/#app.orchestrator.Orchestrator.get_response","title":"<code>get_response(query)</code>","text":"<p>Get the Response for input query. The input query undergoes orchestration to determine the most suitable expert model. Subsequently, the query is processed through the chosen expert model, and the resulting response is returned.   </p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Input Query.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>str</code> <p>Response.</p> Source code in <code>app/orchestrator.py</code> <pre><code>def get_response(self,query):\n    \"\"\"Get the Response for input query.\n    The input query undergoes orchestration to determine the most suitable expert model. Subsequently, the query is processed through the chosen expert model, and the resulting response is returned.   \n\n    Args:\n        query (str): Input Query.\n\n    Returns:\n        response (str): Response.\n    \"\"\"\n    expert_id = self.get_orchestrator_response(query)\n    response = self.get_expert_response(query,expert_id)\n    return response\n</code></pre>"},{"location":"Orchestrator/#app.orchestrator.Orchestrator.load_experts_server","title":"<code>load_experts_server()</code>","text":"<p>Load Servers for Each Expert Activates dedicated machines using specifications from the configuration for each of the Experts' servers.</p> Source code in <code>app/orchestrator.py</code> <pre><code>def load_experts_server(self):\n    \"\"\"\n    Load Servers for Each Expert\n    Activates dedicated machines using specifications from the configuration for each of the Experts' servers.\n    \"\"\"\n    for expert_id,expert in self.experts.items():\n        instance_state = expert.start_server()\n        self.config['logs'].append(f'starting expert {expert_id}')\n</code></pre>"},{"location":"Orchestrator/#app.orchestrator.Orchestrator.load_orchestrator_server","title":"<code>load_orchestrator_server()</code>","text":"<p>Loads the Orchestrator Server. Activates the dedicated machine using specifications from the configuration for the orchestrator server.</p> Source code in <code>app/orchestrator.py</code> <pre><code>def load_orchestrator_server(self):\n    \"\"\"Loads the Orchestrator Server. Activates the dedicated machine using specifications from the configuration for the orchestrator server.\n    \"\"\"\n    if self.orchestrator.config.get('s3_download_path'):\n        raise NotImplementedError\n    instance_state = self.orchestrator.start_server()\n    self.config['logs'].append('starting orchestrator')\n</code></pre>"},{"location":"Orchestrator/#app.orchestrator.Orchestrator.start_inference_endpoints","title":"<code>start_inference_endpoints(max_wait_time=120)</code>","text":"<p>Start Inference Endpoint for Orchestrator and All Experts. The inference endpoints for each model can be served using VLLM or AWS Sagemaker. Note: For closed-source endpoints like GPT-4, <code>start_inference_endpoint</code> is non-functional.</p> <p>Parameters:</p> Name Type Description Default <code>max_wait_time</code> <code>int</code> <p>Defaults to 120.</p> <code>120</code> Source code in <code>app/orchestrator.py</code> <pre><code>def start_inference_endpoints(self, max_wait_time=120):\n    \"\"\"Start Inference Endpoint for Orchestrator and All Experts.\n    The inference endpoints for each model can be served using VLLM or AWS Sagemaker.\n    Note: For closed-source endpoints like GPT-4, `start_inference_endpoint` is non-functional.\n\n    Args:\n        max_wait_time (int, optional): Defaults to 120.\n    \"\"\"\n    self.orchestrator.start_inference_endpoint(max_wait_time)\n    for expert in self.experts.values():\n        expert.start_inference_endpoint(max_wait_time)\n</code></pre>"},{"location":"aws_ami/","title":"VLLM-PYTORCH2-LEEROO","text":""},{"location":"aws_ami/#ami-id-ami-06ff97c8d51fa0dcd","title":"AMI ID: ami-06ff97c8d51fa0dcd","text":""},{"location":"aws_ami/#version-001","title":"Version: 0.0.1","text":""},{"location":"aws_ami/#release-date-29-jan-2024","title":"Release Date: 29 Jan 2024","text":""},{"location":"aws_ami/#overview","title":"Overview","text":"<p>This AWS AMI is specifically designed for serving Hugging Face models. VLLM (Very Large Language Model) is pre-installed for model-serving capabilities. The conda environment : 'pytorch' should be used for running vllm server.  </p>"},{"location":"aws_ami/#ami-details","title":"AMI Details","text":"<ul> <li>AMI ID: ami-06ff97c8d51fa0dcd</li> <li>AMI Name: VLLM-PYTORCH2-LEEROO</li> <li>Base AMI Name: Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) 20240116</li> <li>Architecture: x86_64</li> <li>Release Type: Stable</li> </ul>"},{"location":"aws_ami/#prerequisites","title":"Prerequisites","text":"<ul> <li>Users should have an AWS account.</li> <li>Prepare a security keypair file and a Security Group using your credentials.</li> </ul>"},{"location":"aws_ami/#usage-instructions","title":"Usage Instructions","text":"<p>Please refer : app/test_orchestrator.py  </p>"},{"location":"aws_ami/#usage-instructions-without-leeroo-orch","title":"Usage Instructions (without leeroo orch)","text":"<ol> <li>Launch an EC2 instance using this AMI.</li> <li>Use the security key file created with your credentials for SSH access.</li> <li>Ensure that the security group allows SSH from any IP and has port 8000 open.</li> </ol>"},{"location":"aws_ami/#security-considerations","title":"Security Considerations","text":"<ul> <li>Users must create and use a secure key file for SSH access. Create EC2 key pairs</li> <li>Security group should allow SSH from any IP and have port 8000 open. Create EC2 Security Groups for Linux instances</li> </ul> <p>Thank you for choosing our AWS AMI! If you have any questions or feedback, please don't hesitate to contact our support team at www.leeroo.com.</p>"},{"location":"aws_ec2_utils/","title":"Ec2 Utils","text":""},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.AwsEngine","title":"<code>AwsEngine</code>","text":"<p>AwsEngine uses boto3 for ec2 connections</p> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>class AwsEngine:\n    \"\"\"AwsEngine uses boto3 for ec2 connections\n    \"\"\"\n    def __init__(self,config):\n        self.ec2 = boto3.client(\n            'ec2',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=config.get(\"region\", \"us-east-1\")\n        )\n        self.ec2_resource = boto3.resource(\n            'ec2',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=config.get(\"region\", \"us-east-1\")\n        )\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.check_ec2_server_running_status","title":"<code>check_ec2_server_running_status(aws_engine, InstanceId)</code>","text":"<p>Wait until an ec2 instance is in 'running' state Args:     aws_engine (AwsEngine): aws ec2 connection object.     InstanceId (str): Instance ID.</p> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def check_ec2_server_running_status(\n    aws_engine,\n    InstanceId\n):\n    \"\"\"Wait until an ec2 instance is in 'running' state\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        InstanceId (str): Instance ID.\n    \"\"\"\n    start_time = time.time()\n    time_out = 60*5\n    instance_state = None\n    while True:\n\n        instance_state = get_instance(aws_engine, InstanceId)\n        current_wait_time = (time.time() - start_time)\n        status = \\\n            instance_state['Reservations'][0]['Instances'][0]['State']['Name'] \\\n                == \"running\"\n        if status:\n            print(f\"EC2 Instance Running.\" )\n            break\n        elif current_wait_time &gt; time_out:\n            print(\"TimeOutError : EC2 Instance didnt start!\")\n            break\n        else:\n            print(f\"Current wait time {current_wait_time}.  Max wait time {time_out}\")\n            time.sleep(10)\n\n    return instance_state\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.check_vllm_server_running_status","title":"<code>check_vllm_server_running_status(ssh, tmux_session_name='vllm_server', wait=True, verbose=True, docs_url=None)</code>","text":"<p>Check if VLLM insference endpoint has started.  </p> <p>Parameters:</p> Name Type Description Default <code>ssh</code> <code>SSHClient</code> required <code>tmux_session_name</code> <code>str</code> <p>Defaults to \"vllm_server\".</p> <code>'vllm_server'</code> <code>wait</code> <code>bool</code> <p>Wait unitl the VLLM server is in running state. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <code>True</code> <code>docs_url</code> <code>str</code> <p>This url can to used to check if the endpoint is callable. </p> <code>None</code> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def check_vllm_server_running_status(\n    ssh: paramiko.SSHClient,\n    tmux_session_name: str =\"vllm_server\",\n    wait=True,\n    verbose=True,\n    docs_url=None\n):\n    \"\"\"Check if VLLM insference endpoint has started.  \n\n    Args:\n        ssh (paramiko.SSHClient):    \n        tmux_session_name (str, optional): Defaults to \"vllm_server\".\n        wait (bool, optional): Wait unitl the VLLM server is in running state. Defaults to True.\n        verbose (bool, optional): \n        docs_url (str, optional): This url can to used to check if the endpoint is callable. \n        If this isnt provided, the corresponding tmux pane is used for checks. Defaults to None`\n    \"\"\"\n    start_time = time.time()\n    time_out = 5#sec\n    while True:\n\n        current_wait_time = (time.time() - start_time)\n        if docs_url:\n            try:\n                res = requests.get(docs_url)\n                if res.status_code == 200:\n                    is_running, server_status = True, \"running\"\n            except:\n                is_running, server_status = False, \"connection error\"\n        else:\n            is_running, server_status = is_running_vllm_server(ssh, tmux_session_name, verbose)\n\n        if not wait:\n            return is_running, server_status\n        elif server_status == \"running\":\n            print(f\"VLLM SERVER RUNNING.\" )\n            return server_status\n        elif current_wait_time &gt; time_out:\n            print(\"TimeOutError : vllm server didnt started, Manual intervention needed!\")\n            return server_status\n        else:\n            print(f\"Current wait time {current_wait_time}.  Max wait time {time_out}\")\n            time.sleep(10)\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.create_ec2_instance","title":"<code>create_ec2_instance(aws_engine, expert_instance_name, expert_instance_type='g5.2xlarge', KeyName='connection-key', ami_id='ami-your-ec2-ami-with-vllm-installed')</code>","text":"<p>Created an ec2 instance in the default region.  </p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>expert_instance_name</code> <code>str</code> <p>This will be the instance name</p> required <code>expert_instance_type</code> <code>str</code> <p>Defaults to 'g5.2xlarge'.</p> <code>'g5.2xlarge'</code> <code>KeyName</code> <code>str</code> <p>The pem file key pair to be used. Defaults to 'connection-key'.</p> <code>'connection-key'</code> <code>ami_id</code> <code>str</code> <p>The ami should have vllm installed.                      Defaults to \"ami-your-ec2-ami-with-vllm-installed\".                     This is all the required repo's and pakages installed.                     conda environment to be used is pytorch.</p> <code>'ami-your-ec2-ami-with-vllm-installed'</code> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def create_ec2_instance(\n        aws_engine,\n        expert_instance_name,\n        expert_instance_type='g5.2xlarge',\n        KeyName = 'connection-key',\n        ami_id = \"ami-your-ec2-ami-with-vllm-installed\"\n    ):\n    \"\"\"\n    Created an ec2 instance in the default region.  \n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        expert_instance_name (str): This will be the instance name\n        expert_instance_type (str, optional): Defaults to 'g5.2xlarge'.\n        KeyName (str, optional): The pem file key pair to be used. Defaults to 'connection-key'.\n        ami_id (str, optional): The ami should have vllm installed. \n                                Defaults to \"ami-your-ec2-ami-with-vllm-installed\".\n                                This is all the required repo's and pakages installed.\n                                conda environment to be used is pytorch.\n    \"\"\"\n    if not ami_id:\n        raise\n\n    instance = aws_engine.ec2_resource.create_instances(  \n        ImageId = ami_id,\n        InstanceType = expert_instance_type,\n\n        MinCount=1,\n        MaxCount=1,\n\n        KeyName=KeyName,\n\n        SecurityGroupIds=[\n            # This sequrity group has port 8000 open and allows ssh connection\n             os.getenv('SECURITY_GROUP_ID'),\n        ],\n\n\n        TagSpecifications=[\n            {   'ResourceType': 'instance',\n                'Tags': [\n                    {\n                        'Key': 'Name',\n                        'Value': expert_instance_name\n                    },\n                ]\n            },\n        ],\n    )\n    return instance\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.create_or_revive_expert_instance","title":"<code>create_or_revive_expert_instance(aws_engine, instance_name, model_id, expert_instance_type='g5.2xlarge', KeyName='connection-key', ami_id='ami-your-ec2-ami-with-vllm-installed', tmux_session_name='vllm_server', wait_for_expert_to_start=False, key_path='app/connection-key.pem')</code>","text":"<p>Check if ec2 instace is already present. If alreafy present and is inactive, turn it on Else create a new instance. Connect to the instance and start vllm server.</p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>instance_name</code> <code>str</code> <p>The tag assigned to instance.</p> required <code>model_id</code> <code>str</code> <p>description</p> required <code>expert_instance_type</code> <code>str</code> <p>description. Defaults to 'g5.2xlarge'.  </p> <code>'g5.2xlarge'</code> <code>KeyName</code> <code>str</code> <p>description. Defaults to 'connection-key'.  </p> <code>'connection-key'</code> <code>ami_id</code> <code>str</code> <p>description. Defaults to \"ami-your-ec2-ami-with-vllm-installed\".  </p> <code>'ami-your-ec2-ami-with-vllm-installed'</code> <code>tmux_session_name</code> <code>str</code> <p>Name assigned to tmux session running inference endpoints. Defaults to \"vllm_server\".  </p> <code>'vllm_server'</code> <code>wait_for_expert_to_start</code> <code>bool</code> <p>wait until the expert server is started successfully. It might be recommended to turn this off when threading is not used and experts are started in series. This would eliminate the wait time, and hence the expert checking can be handled outside this function.  </p> <code>False</code> <code>key_path</code> <code>str) </code> <p>Path to the pem file required to connect to ec2 instance.</p> <code>'app/connection-key.pem'</code> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def create_or_revive_expert_instance(\n    aws_engine,\n    instance_name: str,\n    model_id: str,\n    expert_instance_type: str='g5.2xlarge',\n    KeyName: str = 'connection-key',\n    ami_id: str =\"ami-your-ec2-ami-with-vllm-installed\",\n    tmux_session_name: str = \"vllm_server\",\n    wait_for_expert_to_start = False,\n    key_path = \"app/connection-key.pem\"\n):\n    \"\"\"\n    Check if ec2 instace is already present.\n    If alreafy present and is inactive, turn it on\n    Else create a new instance.\n    Connect to the instance and start vllm server.\n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        instance_name (str): The tag assigned to instance.\n        model_id (str): _description_\n        expert_instance_type (str, optional): _description_. Defaults to 'g5.2xlarge'.  \n        KeyName (str, optional): _description_. Defaults to 'connection-key'.  \n        ami_id (str, optional): _description_. Defaults to \"ami-your-ec2-ami-with-vllm-installed\".  \n        tmux_session_name (str, optional): Name assigned to tmux session running inference endpoints. Defaults to \"vllm_server\".  \n        wait_for_expert_to_start (bool): wait until the expert server is started successfully.\n            It might be recommended to turn this off when threading is not used and experts are started in series.  \n            This would eliminate the wait time, and hence the expert checking can be handled outside this function.  \n        key_path (str) : Path to the pem file required to connect to ec2 instance.  \n    \"\"\"\n    # expert_instance_name = modelid_to_instancename(model_id)\n\n    instance_meta = is_ec2_instance_present(aws_engine, instance_name )\n    if instance_meta['is_present'] and \\\n        instance_meta['meta']['State']['Name']=='stopped':\n        print(\"Instance is present and is in stopped state, reviving...\")\n        revive_ec2_instance(aws_engine,instance_meta['meta']['InstanceId'])\n        # instance = check_ec2_server_running_status(aws_engine, instance_meta['meta']['InstanceId'])\n    elif instance_meta['is_present'] and \\\n        instance_meta['meta']['State']['Name'] == 'running':\n        print(\"Instance is present and is in running state...\")\n    elif not instance_meta['is_present']:\n        print(\"Creating new instance...\")\n        \"\"\"Create a new ec2 instance\"\"\"\n        instance = create_ec2_instance(\n            aws_engine,\n            instance_name,\n            expert_instance_type,\n            KeyName,\n            ami_id\n        )\n        instance_meta = is_ec2_instance_present(aws_engine, instance_name )\n        assert instance_meta['is_present']\n    else:\n        print(\"Instance in Initialization mode, please try after sometime.\")\n        return dict()\n\n    \"\"\" connect to instance and start vllm server for the expert \"\"\"\n    try:\n        ip_address = instance_meta['meta']['PublicIpAddress']\n        ssh = get_ssh_session(ip_address, key_path=key_path)\n    except Exception as e:\n        print(\"Instance in Initialization mode, please try after sometime.\")\n        print(str(e))\n        return dict()\n\n    return dict(\n        ip_address=ip_address,\n        instance_name=instance_name\n    )\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.get_available_instances","title":"<code>get_available_instances(aws_engine)</code>","text":"<p>Get the details for all the available instances using aws_engine.</p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def get_available_instances(aws_engine):\n    \"\"\"Get the details for all the available instances using aws_engine.\n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n    \"\"\"\n    response = aws_engine.ec2.describe_instances()\n    available_instances = []\n    for instances in response['Reservations']:\n        for instance in instances['Instances']:\n            available_instances.append(\n                    instance\n                )\n    return available_instances\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.get_instance","title":"<code>get_instance(aws_engine, InstanceId)</code>","text":"<p>Get Metadata for an Instance</p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>InstanceId</code> <code>str</code> <p>Instance ID.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>json</code> <p>Instance metadata</p> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def get_instance(aws_engine,InstanceId):\n    \"\"\"Get Metadata for an Instance\n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        InstanceId (str): Instance ID.\n\n    Returns:\n        response (json): Instance metadata\n    \"\"\"\n    response = aws_engine.ec2.describe_instances(\n        InstanceIds=[InstanceId],\n    )\n    return response\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.is_ec2_instance_present","title":"<code>is_ec2_instance_present(aws_engine, instance_name)</code>","text":"<p>check if an ec2 instance is present</p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>instance_name</code> <code>str</code> <p>The tag assigned to instance.</p> required <p>Returns:     status (dict): Instance Running Status</p> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def is_ec2_instance_present(aws_engine, instance_name):\n    \"\"\"\n    check if an ec2 instance is present\n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        instance_name (str): The tag assigned to instance.\n    Returns:\n        status (dict): Instance Running Status\n    \"\"\"\n    ec2_instances_present = get_available_instances(aws_engine)\n    for instance in ec2_instances_present:\n        for current_instance_name in instance.get('Tags',[]):\n            if current_instance_name['Key'] == \"Name\":\n                if current_instance_name['Value'] == instance_name:\n                    if instance['State']['Name'] != 'terminated':\n                        return dict(\n                            is_present=True, \n                            meta=instance\n                            )\n    return dict( is_present=False, meta=None )\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.revive_ec2_instance","title":"<code>revive_ec2_instance(aws_engine, InstanceId)</code>","text":"<p>Revive an ec2 instance which is in \"Stopped\" state.</p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>InstanceId</code> <code>str</code> <p>Instance ID.</p> required Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def revive_ec2_instance(\n        aws_engine,\n        InstanceId\n):\n    \"\"\"Revive an ec2 instance which is in \"Stopped\" state.\n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        InstanceId (str): Instance ID.\n    \"\"\"\n    response = aws_engine.ec2.start_instances(\n        InstanceIds=[\n            InstanceId,\n        ],\n        DryRun=False\n    )\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.run_vllm_server","title":"<code>run_vllm_server(aws_engine, instance_name, model_id, tmux_session_name='vllm_server', max_wait_time=120, wait_for_expert_to_start=False, key_path='app/keys/connection-key.pem')</code>","text":"<p>Start a VLLM server for the provided model_id. Note model_id should be from HuggingFace.  </p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>instance_name</code> <code>str</code> <p>The tag assigned to instance.</p> required <code>model_id</code> <code>str</code> <p>Huggingface model_id to served.</p> required <code>tmux_session_name</code> <code>str</code> <p>Name assigned to tmux session running inference endpoints. Defaults to \"vllm_server\".  </p> <code>'vllm_server'</code> <code>max_wait_time</code> <code>int</code> <p>Wait time in seconds till the server starts.  </p> <code>120</code> <code>wait_for_expert_to_start</code> <code>bool</code> <p>Pause the Execution until the inference endpoint starts. Defaults to False.  </p> <code>False</code> <code>key_path</code> <code>str) </code> <p>Path to the pem file required to connect to ec2 instance.</p> <code>'app/keys/connection-key.pem'</code> Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def run_vllm_server(\n    aws_engine,\n    instance_name: str,\n    model_id: str,\n    tmux_session_name: str = \"vllm_server\",\n    max_wait_time = 120,\n    wait_for_expert_to_start=False,\n    key_path = \"app/keys/connection-key.pem\"\n):\n    \"\"\"\n    Start a VLLM server for the provided model_id. Note model_id should be from HuggingFace.  \n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        instance_name (str): The tag assigned to instance.\n        model_id (str): Huggingface model_id to served.\n        tmux_session_name (str, optional): Name assigned to tmux session running inference endpoints. Defaults to \"vllm_server\".  \n        max_wait_time (int, optional): Wait time in seconds till the server starts.  \n        wait_for_expert_to_start (bool, optional): Pause the Execution until the inference endpoint starts. Defaults to False.  \n        key_path (str) : Path to the pem file required to connect to ec2 instance.  \n    \"\"\"\n\n    \"\"\" connect to instance and start vllm server for the expert \"\"\"\n    start_time = time.time()\n    while True:\n        try:\n            instance_meta = is_ec2_instance_present(aws_engine, instance_name )\n            ip_address = instance_meta['meta']['PublicIpAddress']\n            ssh = get_ssh_session(ip_address, key_path=key_path)\n            break\n        except:\n            pass\n        if time.time() - start_time &gt; max_wait_time:\n            print(\"Unable to ssh to instance, instace might be initializing, please try after some time or increase the max_wait_time\")\n            return\n        else:\n            time.sleep(15)\n\n    start_vllm_server(\n            ssh = ssh,\n            model_id = model_id,\n            ip_address = ip_address,\n            port =8000,\n            conda_env_name = \"pytorch\",\n            tmux_session_name = tmux_session_name\n        )\n\n    \"\"\" keep waiting till expert starts \"\"\"\n    if wait_for_expert_to_start:\n        check_vllm_server_running_status( ssh, tmux_session_name )\n\n    response = dict(ip_address=ip_address,instance_name=instance_name)\n    response.update(instance_meta['meta'])\n    return response\n</code></pre>"},{"location":"aws_ec2_utils/#app.utils.aws_ec2_utils.stop_ec2_instance","title":"<code>stop_ec2_instance(aws_engine, InstanceId)</code>","text":"<p>Stop an Ec2 Instance. NOTE:  This would change the status of ec2 instance to \"stopped\" and not \"Terminate\".  </p> <p>Parameters:</p> Name Type Description Default <code>aws_engine</code> <code>AwsEngine</code> <p>aws ec2 connection object.</p> required <code>InstanceId</code> <code>str</code> <p>Instance ID.</p> required Source code in <code>app/utils/aws_ec2_utils.py</code> <pre><code>def stop_ec2_instance(\n        aws_engine,\n        InstanceId\n):\n    \"\"\"Stop an Ec2 Instance.\n    NOTE:  This would change the status of ec2 instance to \"stopped\" and not \"Terminate\".  \n\n    Args:\n        aws_engine (AwsEngine): aws ec2 connection object.\n        InstanceId (str): Instance ID.\n    \"\"\"\n    response = aws_engine.ec2.stop_instances(\n        InstanceIds=[\n            InstanceId,\n        ],\n        DryRun=False\n    )\n    return response\n</code></pre>"},{"location":"aws_ec2_vllm/","title":"AWS EC2 VLLM","text":"<p>             Bases: <code>BaseServerObject</code></p> <p>Serve an Expert Model using VLLM on AWS EC2 Machine Expert models operate independently and can be hosted on different or common machines.  They may include closed-source models like GPT-4. Refer to (app/configs/demo_orch_ec2_mix.json) for configuration details.</p> Source code in <code>app/servers/vllm_ec2_server.py</code> <pre><code>class Ec2VllmExpert(BaseServerObject):\n    \"\"\"\n        Serve an Expert Model using VLLM on AWS EC2 Machine\n        Expert models operate independently and can be hosted on different or common machines. \n        They may include closed-source models like GPT-4. Refer to (app/configs/demo_orch_ec2_mix.json) for configuration details.\n    \"\"\"\n    def __init__(self,**kwargs):\n        \"\"\"\n        \"\"\"\n        self.config = kwargs\n        self.aws_engine = AwsEngine(self.config)\n        self.instance_name = self.config.get('instance_name')\n        self.model_id = self.config.get('model_id')\n        self.base_url = None\n        self.config.update(dict(logs=[]))\n\n    def start_server(self):\n        \"\"\"\n        Starts a Dedicated EC2 Instance\n        The 'instance_name' serves as a unique identifier. If an instance tagged with 'instance_name' is already present in a given region, the operation is aborted. The unique identifier can be edited in app/server_manager.py: get_server.\n        \"\"\"\n        instance_state = \\\n            create_or_revive_expert_instance(\n                self.aws_engine,\n                instance_name=self.instance_name,\n                model_id=self.model_id,\n                expert_instance_type=self.config['instance_type'],\n                KeyName=self.config['KeyName'],\n                ami_id=self.config['ami_id'],\n                wait_for_expert_to_start=False,\n                key_path = self.config['KeyPath']\n            )\n        self.config.update(instance_state)\n        self.config['logs'].append(f'starting expert server {self.model_id}')\n\n    def _set_base_url(self):\n        self.ip_address = self.config.get('ip_address')\n        self.port = self.config.get('port', 8000)\n        self.transfer_protocol =  self.config.get('transfer_protocol', 'http')\n        self.base_url = \\\n            f\"{self.transfer_protocol}://{self.ip_address}:{self.port}/v1\"\n        self.docs_url = \\\n            f\"{self.transfer_protocol}://{self.ip_address}:{self.port}/docs\"\n\n    def start_inference_endpoint(self, max_wait_time=120):\n        \"\"\"Starts a new tumx session with name 'tmux_session_name' and activates the environment 'pytorch'.        \n        NOTE: we provide an aws ami that has the required conda env and VLLM installed and ready to used. For changing the env refer app/utils/ssh_utils.py : start_vllm_server\n\n        Args:\n            max_wait_time (int, optional): Defaults to 120.\n        \"\"\"\n        instance_meta = \\\n            run_vllm_server(\n                    self.aws_engine,\n                    self.instance_name,\n                    self.model_id,\n                    max_wait_time = max_wait_time,\n                    wait_for_expert_to_start=False,\n                    key_path= self.config['KeyPath']\n                )\n        self.config.update(instance_meta)\n        self._set_base_url()\n        self.config['logs'].append(f'starting expert inference endpoint {self.model_id}')\n\n    def stop_server(self):\n        \"\"\"Stops the Ec2 server. \n        \"\"\"\n        response = stop_ec2_instance(\n            self.aws_engine, \n            self.config['InstanceId']\n        )\n        return response\n\n    def check_servers_state(self):\n        ssh = get_ssh_session(self.config['ip_address'], key_path=self.config['KeyPath'])\n        status = check_vllm_server_running_status(\n                ssh, wait=False, verbose=False, docs_url=self.docs_url)\n        return status\n\n    def get_response(self, message, stream=False):\n        \"\"\"Generate Text using the expert LLM.\n\n        Args:\n            message (str): Input Query\n            stream (bool, optional): Get a response stream. Defaults to False.\n\n        Returns:\n            response (str): Output Text Generation\n        \"\"\"\n        messages = [{\"role\": \"user\", \"content\": message}]\n        if stream:\n            return self._generate_stream(messages)\n        else:\n            return self._generate(messages)\n\n    def _generate(self, messages, max_new_tokens=1000):\n        openai.api_base = self.base_url\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model_id,\n                messages=messages,\n                request_timeout=60\n            )\n            answer = response['choices'][0]['message']['content']\n            return answer\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return \"Sorry, I couldn't process your request. Too many requests for me to handle!\"\n\n    def _generate_stream(self, messages, max_new_tokens=1000):\n        openai.api_base = self.base_url\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model_id,\n                messages=messages,\n                request_timeout=60,\n                stream=True\n            )\n            for chunk in response:\n                content = chunk['choices'][0]['delta'].get(\"content\", \"\")\n                yield content\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            yield \"Sorry, I couldn't process your request. Too many requests for me to handle!\"\n</code></pre>"},{"location":"aws_ec2_vllm/#app.servers.vllm_ec2_server.Ec2VllmExpert.__init__","title":"<code>__init__(**kwargs)</code>","text":"Source code in <code>app/servers/vllm_ec2_server.py</code> <pre><code>def __init__(self,**kwargs):\n    \"\"\"\n    \"\"\"\n    self.config = kwargs\n    self.aws_engine = AwsEngine(self.config)\n    self.instance_name = self.config.get('instance_name')\n    self.model_id = self.config.get('model_id')\n    self.base_url = None\n    self.config.update(dict(logs=[]))\n</code></pre>"},{"location":"aws_ec2_vllm/#app.servers.vllm_ec2_server.Ec2VllmExpert.get_response","title":"<code>get_response(message, stream=False)</code>","text":"<p>Generate Text using the expert LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Input Query</p> required <code>stream</code> <code>bool</code> <p>Get a response stream. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>str</code> <p>Output Text Generation</p> Source code in <code>app/servers/vllm_ec2_server.py</code> <pre><code>def get_response(self, message, stream=False):\n    \"\"\"Generate Text using the expert LLM.\n\n    Args:\n        message (str): Input Query\n        stream (bool, optional): Get a response stream. Defaults to False.\n\n    Returns:\n        response (str): Output Text Generation\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": message}]\n    if stream:\n        return self._generate_stream(messages)\n    else:\n        return self._generate(messages)\n</code></pre>"},{"location":"aws_ec2_vllm/#app.servers.vllm_ec2_server.Ec2VllmExpert.start_inference_endpoint","title":"<code>start_inference_endpoint(max_wait_time=120)</code>","text":"<p>Starts a new tumx session with name 'tmux_session_name' and activates the environment 'pytorch'.       NOTE: we provide an aws ami that has the required conda env and VLLM installed and ready to used. For changing the env refer app/utils/ssh_utils.py : start_vllm_server</p> <p>Parameters:</p> Name Type Description Default <code>max_wait_time</code> <code>int</code> <p>Defaults to 120.</p> <code>120</code> Source code in <code>app/servers/vllm_ec2_server.py</code> <pre><code>def start_inference_endpoint(self, max_wait_time=120):\n    \"\"\"Starts a new tumx session with name 'tmux_session_name' and activates the environment 'pytorch'.        \n    NOTE: we provide an aws ami that has the required conda env and VLLM installed and ready to used. For changing the env refer app/utils/ssh_utils.py : start_vllm_server\n\n    Args:\n        max_wait_time (int, optional): Defaults to 120.\n    \"\"\"\n    instance_meta = \\\n        run_vllm_server(\n                self.aws_engine,\n                self.instance_name,\n                self.model_id,\n                max_wait_time = max_wait_time,\n                wait_for_expert_to_start=False,\n                key_path= self.config['KeyPath']\n            )\n    self.config.update(instance_meta)\n    self._set_base_url()\n    self.config['logs'].append(f'starting expert inference endpoint {self.model_id}')\n</code></pre>"},{"location":"aws_ec2_vllm/#app.servers.vllm_ec2_server.Ec2VllmExpert.start_server","title":"<code>start_server()</code>","text":"<p>Starts a Dedicated EC2 Instance The 'instance_name' serves as a unique identifier. If an instance tagged with 'instance_name' is already present in a given region, the operation is aborted. The unique identifier can be edited in app/server_manager.py: get_server.</p> Source code in <code>app/servers/vllm_ec2_server.py</code> <pre><code>def start_server(self):\n    \"\"\"\n    Starts a Dedicated EC2 Instance\n    The 'instance_name' serves as a unique identifier. If an instance tagged with 'instance_name' is already present in a given region, the operation is aborted. The unique identifier can be edited in app/server_manager.py: get_server.\n    \"\"\"\n    instance_state = \\\n        create_or_revive_expert_instance(\n            self.aws_engine,\n            instance_name=self.instance_name,\n            model_id=self.model_id,\n            expert_instance_type=self.config['instance_type'],\n            KeyName=self.config['KeyName'],\n            ami_id=self.config['ami_id'],\n            wait_for_expert_to_start=False,\n            key_path = self.config['KeyPath']\n        )\n    self.config.update(instance_state)\n    self.config['logs'].append(f'starting expert server {self.model_id}')\n</code></pre>"},{"location":"aws_ec2_vllm/#app.servers.vllm_ec2_server.Ec2VllmExpert.stop_server","title":"<code>stop_server()</code>","text":"<p>Stops the Ec2 server.</p> Source code in <code>app/servers/vllm_ec2_server.py</code> <pre><code>def stop_server(self):\n    \"\"\"Stops the Ec2 server. \n    \"\"\"\n    response = stop_ec2_instance(\n        self.aws_engine, \n        self.config['InstanceId']\n    )\n    return response\n</code></pre>"},{"location":"aws_sagemaker/","title":"AWS Sagemaker","text":"<p>             Bases: <code>BaseServerObject</code></p> <p>Serve an Expert Model using AWS Sagemaker Inference. Expert models operate independently and can be hosted on different or common machines.  They may include closed-source models like GPT-4. Refer to (app/configs/demo_orch_ec2_mix.json) for configuration details.</p> Source code in <code>app/servers/sagemaker_server.py</code> <pre><code>class SagemakerServer(BaseServerObject):\n    \"\"\"\n        Serve an Expert Model using AWS Sagemaker Inference.\n        Expert models operate independently and can be hosted on different or common machines. \n        They may include closed-source models like GPT-4. Refer to (app/configs/demo_orch_ec2_mix.json) for configuration details.\n    \"\"\"\n    def __init__(self,**kwargs):\n        \"\"\"\n        \"\"\"\n        self.config = kwargs\n\n        os.environ['AWS_DEFAULT_REGION'] = self.config.get(\"region\", \"us-east-1\")\n\n        self.engine = AwsSagemakerEngine(self.config)\n        self.model_id = self.config['model_id']\n        self.instance_name = self.model_id\n        self._set_inferece_params()\n        self.config.update(dict(logs=[]))\n\n    def _set_inferece_params(self):\n        self.inference_params = {\n            \"top_p\": self.config.get(\"top_p\", 0.6),\n            \"top_k\": self.config.get(\"top_k\", 50),\n            \"stop\": self.config.get(\"stop\", [\"&lt;/s&gt;\"]),\n            \"do_sample\": self.config.get(\"do_sample\", True),\n            \"temperature\": self.config.get(\"temperature\", 0.9),\n            \"max_new_tokens\": self.config.get(\"max_new_tokens\", 512),\n            \"return_full_text\": self.config.get(\"return_full_text\", False),\n            \"repetition_penalty\": self.config.get(\"repetition_penalty\", 1.03)\n        }\n\n    def get_endpoint_name(self):\n        \"\"\"HACK : endpoint names with more than 2 '-' are not supported 28jan2024.\n        We use model_id for creating endpoint name and hence trim the later\"\"\"\n        endpoint_name = \"-\".join(self.config['model_id'].split(\"-\")[0:2])\n        endpoint_name = f\"{endpoint_name.split('/')[1]}-tgi-streaming\"\n        return endpoint_name\n\n    def start_server(self):\n        \"\"\"Since aws sagemaker deployment dosent require this step, 'start_inference_endpoint' covers everything.\n        \"\"\"\n        self.config['logs'].append(f'Sagemaker server {self.model_id}')\n\n    def start_inference_endpoint(self, max_wait_time=600):\n        \"\"\"\n        Starts an AWS SageMaker Endpoint using the 'instance_type' from the configuration.\n        The 'get_endpoint_name' method returns a unique identifier for the expert. If an inference endpoint with the same name is already present in the provided region, the operation is aborted.\n        NOTE: All models available on Hugging Face can be served using AWS SageMaker.\n\n        Args:\n            max_wait_time (int, optional): Defaults to 600.\n        \"\"\"\n\n        \"\"\"Check if inference endpoint is already present\"\"\"\n        is_present = False\n        instance_meta = {}\n        endpoints = self.engine.sagemaker_client.list_endpoints()\n        model_id_trimmed =  self.get_endpoint_name()\n        for endpoint in endpoints['Endpoints']:\n            if model_id_trimmed in endpoint['EndpointName'] and \\\n                endpoint['EndpointStatus'] in ['Creating', 'InService']:\n                \"\"\"TODO the inference endpoint matching strategy is using the model_id\n                In case of multiple experts with same model_id assign tags during endpoint creation\n                and use that as a filter here\"\"\"\n                is_present = True\n                self.endpoint_name = endpoint['EndpointName']\n                instance_meta = dict(\n                    is_present=is_present,\n                    ip_address=\"sagemaker-endpoint\",\n                    instance_name=self.endpoint_name,\n                    endpoint_name=self.endpoint_name\n                )\n        self.config.update(instance_meta)\n        if is_present:\n            return\n\n        \"\"\"Create a new inference endpoint\"\"\"\n        llm_image = get_hf_image(self.config.get(\"region\", \"us-east-1\"))\n        self.llm_image = llm_image.__str__()\n\n        # sagemaker config\n        instance_type = self.config['instance_type']\n        number_of_gpu = self.config.get(\"number_of_gpu\", 1)\n\n        # Define Model and Endpoint configuration parameters\n        config = {\n            'HF_MODEL_ID': self.model_id, # model_id from hf.co/models\n            'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n            'MAX_INPUT_LENGTH': json.dumps(self.config.get(\"max_input_length\", 2048)),  # Max length of input text\n            'MAX_TOTAL_TOKENS': json.dumps(self.config.get(\"max_total_length\",4096)),  # Max length of the generation (including input text)\n            'MAX_BATCH_TOTAL_TOKENS': json.dumps(self.config.get(\"max_batch_total_tokens\",8192)),  # Limits the number of tokens that can be processed in parallel during the generation\n            'HUGGING_FACE_HUB_TOKEN': os.getenv(\"HUGGING_FACE_HUB_TOKEN\") # Read Access token of your HuggingFace profile https://huggingface.co/settings/tokens\n        }\n\n        # create HuggingFaceModel with the image uri\n        llm_model = HuggingFaceModel(\n            role=self.engine.role,\n            image_uri=self.llm_image,\n            env=config\n        )\n\n        endpoint_name = self.get_endpoint_name()\n        self.endpoint_name = name_from_base(endpoint_name)\n        print(self.endpoint_name)\n\n        llm = llm_model.deploy(\n            endpoint_name=self.endpoint_name,\n            initial_instance_count=1,\n            instance_type=instance_type,\n            wait=False, # Whether the call should wait until the deployment of this model completes\n            container_startup_health_check_timeout=max_wait_time,\n        )\n\n        instance_meta = dict(\n            ip_address=\"sagemaker-endpoint\",\n            instance_name=self.endpoint_name,\n            endpoint_name=self.endpoint_name\n        )\n        self.config.update(instance_meta)\n        self.config['logs'].append(f'Sagemaker inference endpoint {self.model_id}')\n\n    def stop_server(self):\n        \"\"\"Terminate the Sagemaker Endpoint\n        \"\"\"\n        endpoint = self.engine.sagemaker_client.describe_endpoint(\n            EndpointName=self.endpoint_name)\n        endpoint_config_name = endpoint['EndpointConfigName']\n        endpoint_config = self.engine.sagemaker_client.describe_endpoint_config(\n            EndpointConfigName=endpoint_config_name)\n        model_name = endpoint_config['ProductionVariants'][0]['ModelName']\n\n        print(f\"\"\"\n            About to delete the following sagemaker resources:\n            Endpoint: {self.endpoint_name}\n            Endpoint Config: {endpoint_config_name}\n            Model: {model_name}\n            \"\"\")    \n        # delete endpoint\n        self.engine.sagemaker_client.delete_endpoint(EndpointName=self.endpoint_name)\n        # delete endpoint config\n        self.engine.sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n        # delete model\n        self.engine.sagemaker_client.delete_model(ModelName=model_name)\n\n    def _get_inference_payload(self, message, stream=True):\n        payload = {\n            \"inputs\":  message,\n            \"parameters\": self.inference_params,\n            \"stream\": stream\n        }\n        return payload\n\n    def check_servers_state(self):\n        payload = self._get_inference_payload(\"hello!\", stream=True)\n        try:\n            resp = get_realtime_response_stream(\n                self.engine.sagemaker_runtime, \n                self.endpoint_name, \n                payload\n            )\n        except:\n            return (False, '')\n        # print_response_stream(resp)\n        return (True, 'running')\n\n    def get_response(self, message, stream=True, verbose=True):\n        payload = self._get_inference_payload(message, stream)\n        resp = get_realtime_response_stream(\n            self.engine.sagemaker_runtime, \n            self.endpoint_name, \n            payload\n        )\n        if stream:\n            text = parse_response_stream(resp, verbose)\n        else: # TODO parse the response generated when streaming is false\n            raise NotImplementedError\n        return text\n</code></pre>"},{"location":"aws_sagemaker/#app.servers.sagemaker_server.SagemakerServer.__init__","title":"<code>__init__(**kwargs)</code>","text":"Source code in <code>app/servers/sagemaker_server.py</code> <pre><code>def __init__(self,**kwargs):\n    \"\"\"\n    \"\"\"\n    self.config = kwargs\n\n    os.environ['AWS_DEFAULT_REGION'] = self.config.get(\"region\", \"us-east-1\")\n\n    self.engine = AwsSagemakerEngine(self.config)\n    self.model_id = self.config['model_id']\n    self.instance_name = self.model_id\n    self._set_inferece_params()\n    self.config.update(dict(logs=[]))\n</code></pre>"},{"location":"aws_sagemaker/#app.servers.sagemaker_server.SagemakerServer.get_endpoint_name","title":"<code>get_endpoint_name()</code>","text":"<p>HACK : endpoint names with more than 2 '-' are not supported 28jan2024. We use model_id for creating endpoint name and hence trim the later</p> Source code in <code>app/servers/sagemaker_server.py</code> <pre><code>def get_endpoint_name(self):\n    \"\"\"HACK : endpoint names with more than 2 '-' are not supported 28jan2024.\n    We use model_id for creating endpoint name and hence trim the later\"\"\"\n    endpoint_name = \"-\".join(self.config['model_id'].split(\"-\")[0:2])\n    endpoint_name = f\"{endpoint_name.split('/')[1]}-tgi-streaming\"\n    return endpoint_name\n</code></pre>"},{"location":"aws_sagemaker/#app.servers.sagemaker_server.SagemakerServer.start_inference_endpoint","title":"<code>start_inference_endpoint(max_wait_time=600)</code>","text":"<p>Starts an AWS SageMaker Endpoint using the 'instance_type' from the configuration. The 'get_endpoint_name' method returns a unique identifier for the expert. If an inference endpoint with the same name is already present in the provided region, the operation is aborted. NOTE: All models available on Hugging Face can be served using AWS SageMaker.</p> <p>Parameters:</p> Name Type Description Default <code>max_wait_time</code> <code>int</code> <p>Defaults to 600.</p> <code>600</code> Source code in <code>app/servers/sagemaker_server.py</code> <pre><code>def start_inference_endpoint(self, max_wait_time=600):\n    \"\"\"\n    Starts an AWS SageMaker Endpoint using the 'instance_type' from the configuration.\n    The 'get_endpoint_name' method returns a unique identifier for the expert. If an inference endpoint with the same name is already present in the provided region, the operation is aborted.\n    NOTE: All models available on Hugging Face can be served using AWS SageMaker.\n\n    Args:\n        max_wait_time (int, optional): Defaults to 600.\n    \"\"\"\n\n    \"\"\"Check if inference endpoint is already present\"\"\"\n    is_present = False\n    instance_meta = {}\n    endpoints = self.engine.sagemaker_client.list_endpoints()\n    model_id_trimmed =  self.get_endpoint_name()\n    for endpoint in endpoints['Endpoints']:\n        if model_id_trimmed in endpoint['EndpointName'] and \\\n            endpoint['EndpointStatus'] in ['Creating', 'InService']:\n            \"\"\"TODO the inference endpoint matching strategy is using the model_id\n            In case of multiple experts with same model_id assign tags during endpoint creation\n            and use that as a filter here\"\"\"\n            is_present = True\n            self.endpoint_name = endpoint['EndpointName']\n            instance_meta = dict(\n                is_present=is_present,\n                ip_address=\"sagemaker-endpoint\",\n                instance_name=self.endpoint_name,\n                endpoint_name=self.endpoint_name\n            )\n    self.config.update(instance_meta)\n    if is_present:\n        return\n\n    \"\"\"Create a new inference endpoint\"\"\"\n    llm_image = get_hf_image(self.config.get(\"region\", \"us-east-1\"))\n    self.llm_image = llm_image.__str__()\n\n    # sagemaker config\n    instance_type = self.config['instance_type']\n    number_of_gpu = self.config.get(\"number_of_gpu\", 1)\n\n    # Define Model and Endpoint configuration parameters\n    config = {\n        'HF_MODEL_ID': self.model_id, # model_id from hf.co/models\n        'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n        'MAX_INPUT_LENGTH': json.dumps(self.config.get(\"max_input_length\", 2048)),  # Max length of input text\n        'MAX_TOTAL_TOKENS': json.dumps(self.config.get(\"max_total_length\",4096)),  # Max length of the generation (including input text)\n        'MAX_BATCH_TOTAL_TOKENS': json.dumps(self.config.get(\"max_batch_total_tokens\",8192)),  # Limits the number of tokens that can be processed in parallel during the generation\n        'HUGGING_FACE_HUB_TOKEN': os.getenv(\"HUGGING_FACE_HUB_TOKEN\") # Read Access token of your HuggingFace profile https://huggingface.co/settings/tokens\n    }\n\n    # create HuggingFaceModel with the image uri\n    llm_model = HuggingFaceModel(\n        role=self.engine.role,\n        image_uri=self.llm_image,\n        env=config\n    )\n\n    endpoint_name = self.get_endpoint_name()\n    self.endpoint_name = name_from_base(endpoint_name)\n    print(self.endpoint_name)\n\n    llm = llm_model.deploy(\n        endpoint_name=self.endpoint_name,\n        initial_instance_count=1,\n        instance_type=instance_type,\n        wait=False, # Whether the call should wait until the deployment of this model completes\n        container_startup_health_check_timeout=max_wait_time,\n    )\n\n    instance_meta = dict(\n        ip_address=\"sagemaker-endpoint\",\n        instance_name=self.endpoint_name,\n        endpoint_name=self.endpoint_name\n    )\n    self.config.update(instance_meta)\n    self.config['logs'].append(f'Sagemaker inference endpoint {self.model_id}')\n</code></pre>"},{"location":"aws_sagemaker/#app.servers.sagemaker_server.SagemakerServer.start_server","title":"<code>start_server()</code>","text":"<p>Since aws sagemaker deployment dosent require this step, 'start_inference_endpoint' covers everything.</p> Source code in <code>app/servers/sagemaker_server.py</code> <pre><code>def start_server(self):\n    \"\"\"Since aws sagemaker deployment dosent require this step, 'start_inference_endpoint' covers everything.\n    \"\"\"\n    self.config['logs'].append(f'Sagemaker server {self.model_id}')\n</code></pre>"},{"location":"aws_sagemaker/#app.servers.sagemaker_server.SagemakerServer.stop_server","title":"<code>stop_server()</code>","text":"<p>Terminate the Sagemaker Endpoint</p> Source code in <code>app/servers/sagemaker_server.py</code> <pre><code>def stop_server(self):\n    \"\"\"Terminate the Sagemaker Endpoint\n    \"\"\"\n    endpoint = self.engine.sagemaker_client.describe_endpoint(\n        EndpointName=self.endpoint_name)\n    endpoint_config_name = endpoint['EndpointConfigName']\n    endpoint_config = self.engine.sagemaker_client.describe_endpoint_config(\n        EndpointConfigName=endpoint_config_name)\n    model_name = endpoint_config['ProductionVariants'][0]['ModelName']\n\n    print(f\"\"\"\n        About to delete the following sagemaker resources:\n        Endpoint: {self.endpoint_name}\n        Endpoint Config: {endpoint_config_name}\n        Model: {model_name}\n        \"\"\")    \n    # delete endpoint\n    self.engine.sagemaker_client.delete_endpoint(EndpointName=self.endpoint_name)\n    # delete endpoint config\n    self.engine.sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n    # delete model\n    self.engine.sagemaker_client.delete_model(ModelName=model_name)\n</code></pre>"},{"location":"aws_sagemaker_utils/","title":"Sagemaker Utils","text":""},{"location":"aws_sagemaker_utils/#app.utils.aws_sagemaker_utils.AwsSagemakerEngine","title":"<code>AwsSagemakerEngine</code>","text":"<p>AwsEngine uses boto3 for sagemaker connections</p> Source code in <code>app/utils/aws_sagemaker_utils.py</code> <pre><code>class AwsSagemakerEngine:\n    \"\"\"AwsEngine uses boto3 for sagemaker connections\n    \"\"\"\n    def __init__(self,config):\n        self.iam = boto3.client(\n            'iam',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=config.get(\"region\", \"us-east-1\")\n        )\n        self.role = self.iam.get_role(\n            RoleName=os.getenv('AWS_SAGEMAKER_ROLE_NAME'))['Role']['Arn']\n\n        self.sagemaker_runtime = boto3.client(\n            'sagemaker-runtime',         \n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=config.get(\"region\", \"us-east-1\")\n        )\n        self.sagemaker_client = boto3.client(\n            'sagemaker',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=config.get(\"region\", \"us-east-1\")\n        )\n</code></pre>"},{"location":"aws_sagemaker_utils/#app.utils.aws_sagemaker_utils.LineIterator","title":"<code>LineIterator</code>","text":"<p>A helper class for parsing the byte stream input. </p> <p>The output of the model will be in the following format:</p> <pre><code>{\"outputs\": [\" a\"]}   \n{\"outputs\": [\" challenging\"]}   \n{\"outputs\": [\" problem\"]}   \n...\n</code></pre> <p>While usually each PayloadPart event from the event stream will contain a byte array  with a full json, this is not guaranteed and some of the json objects may be split across PayloadPart events. For example:</p> <pre><code>{'PayloadPart': {'Bytes': {\"outputs\": '}}   \n{'PayloadPart': {'Bytes': [\" problem\"]}'}}\n</code></pre> <p>This class accounts for this by concatenating bytes written via the 'write' function and then exposing a method which will return lines (ending with a '\\n' character) within the buffer via the 'scan_lines' function. It maintains the position of the last read  position to ensure that previous bytes are not exposed again.</p> Source code in <code>app/utils/aws_sagemaker_utils.py</code> <pre><code>class LineIterator:\n    \"\"\"\n    A helper class for parsing the byte stream input. \n\n    The output of the model will be in the following format:\n\n        {\"outputs\": [\" a\"]}   \n        {\"outputs\": [\" challenging\"]}   \n        {\"outputs\": [\" problem\"]}   \n        ...\n\n\n    While usually each PayloadPart event from the event stream will contain a byte array \n    with a full json, this is not guaranteed and some of the json objects may be split across\n    PayloadPart events. For example:\n\n        {'PayloadPart': {'Bytes': {\"outputs\": '}}   \n        {'PayloadPart': {'Bytes': [\" problem\"]}'}}   \n\n\n    This class accounts for this by concatenating bytes written via the 'write' function\n    and then exposing a method which will return lines (ending with a '\\\\n' character) within\n    the buffer via the 'scan_lines' function. It maintains the position of the last read \n    position to ensure that previous bytes are not exposed again. \n    \"\"\"\n\n    def __init__(self, stream):\n        self.byte_iterator = iter(stream)\n        self.buffer = io.BytesIO()\n        self.read_pos = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        while True:\n            self.buffer.seek(self.read_pos)\n            line = self.buffer.readline()\n            if line and line[-1] == ord('\\n'):\n                self.read_pos += len(line)\n                return line[:-1]\n            try:\n                chunk = next(self.byte_iterator)\n            except StopIteration:\n                if self.read_pos &lt; self.buffer.getbuffer().nbytes:\n                    continue\n                raise\n            if 'PayloadPart' not in chunk:\n                print('Unknown event type:' + chunk)\n                continue\n            self.buffer.seek(0, io.SEEK_END)\n            self.buffer.write(chunk['PayloadPart']['Bytes'])\n</code></pre>"},{"location":"aws_sagemaker_utils/#app.utils.aws_sagemaker_utils.get_realtime_response_stream","title":"<code>get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload)</code>","text":"<p>Fetch Streaming Text Generation response from AWS Sagemaker Enpoint.  </p> <p>Parameters:</p> Name Type Description Default <code>sagemaker_runtime</code> <code>sagemaker_runtime</code> required <code>endpoint_name</code> <code>str</code> <p>Endpoint Name.</p> required <code>payload</code> <code>json</code> <p>Request</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>app/utils/aws_sagemaker_utils.py</code> <pre><code>def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n    \"\"\"Fetch Streaming Text Generation response from AWS Sagemaker Enpoint.  \n\n    Args:\n        sagemaker_runtime (AwsSagemakerEngine.sagemaker_runtime):   \n        endpoint_name (str): Endpoint Name.\n        payload (json): Request\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        Body=json.dumps(payload), \n        ContentType=\"application/json\",\n        CustomAttributes='accept_eula=true'\n    )\n    return response_stream\n</code></pre>"},{"location":"aws_sagemaker_utils/#app.utils.aws_sagemaker_utils.parse_response_stream","title":"<code>parse_response_stream(response_stream, verbose=True)</code>","text":"<p>Prints Streaming Text</p> <p>Parameters:</p> Name Type Description Default <code>response_stream</code> <p>response stream</p> required <code>verbose</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>resonse</code> <code>str</code> <p>Generated Text.</p> Source code in <code>app/utils/aws_sagemaker_utils.py</code> <pre><code>def parse_response_stream(response_stream, verbose=True):\n    \"\"\"Prints Streaming Text\n\n    Args:\n        response_stream : response stream\n        verbose (bool, optional): _description_. Defaults to True.\n\n    Returns:\n        resonse (str): Generated Text.\n    \"\"\"\n    response_text = \"\"\n    event_stream = response_stream['Body']\n    start_json = b'{'\n    stop_token = '&lt;/s&gt;'\n    for line in LineIterator(event_stream):\n        if line != b'' and start_json in line:\n            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n            if data['token']['text'] != stop_token:\n                response_text += data['token']['text']\n                if verbose: print(data['token']['text'],end='')\n    return response_text  \n</code></pre>"},{"location":"demo_server/","title":"Dummy Demo Server","text":"<p>             Bases: <code>BaseServerObject</code></p> <p>This is a Test Server used for Debugging Orchestrator flow.</p> Source code in <code>app/servers/dummy_server.py</code> <pre><code>class DummyLocalOrchestrator(BaseServerObject):\n    \"\"\"\n        This is a Test Server used for Debugging Orchestrator flow.\n    \"\"\"\n    def __init__(self,**kwargs):\n        \"\"\"\n        \"\"\"\n        self.config = kwargs\n        self.model_id = self.config.get(\"model_id\")\n        self.instance_name = self.config.get(\"model_id\")\n        self.config.update(dict(logs=[]))\n\n    def start_server(self):\n        self.config['logs'].append(f'Dummy Orchestrator server {self.model_id}')\n\n    def start_inference_endpoint(self, max_wait_time=120):\n        self.config['logs'].append(f'Dummy Orchestrator inference endpoint {self.model_id}')\n\n    def stop_server(self):\n        return \"server stopped\"\n\n    def check_servers_state(self):\n        return (True, 'running')\n\n    def get_response(self, message, stream=False):\n        \"\"\"Since this is a dummy orchestrator for testing, \n        it would return random expert id for each of the input query\"\"\"\n        return random.randint(0,2)         \n</code></pre>"},{"location":"demo_server/#app.servers.dummy_server.DummyLocalOrchestrator.__init__","title":"<code>__init__(**kwargs)</code>","text":"Source code in <code>app/servers/dummy_server.py</code> <pre><code>def __init__(self,**kwargs):\n    \"\"\"\n    \"\"\"\n    self.config = kwargs\n    self.model_id = self.config.get(\"model_id\")\n    self.instance_name = self.config.get(\"model_id\")\n    self.config.update(dict(logs=[]))\n</code></pre>"},{"location":"demo_server/#app.servers.dummy_server.DummyLocalOrchestrator.get_response","title":"<code>get_response(message, stream=False)</code>","text":"<p>Since this is a dummy orchestrator for testing,  it would return random expert id for each of the input query</p> Source code in <code>app/servers/dummy_server.py</code> <pre><code>def get_response(self, message, stream=False):\n    \"\"\"Since this is a dummy orchestrator for testing, \n    it would return random expert id for each of the input query\"\"\"\n    return random.randint(0,2)         \n</code></pre>"},{"location":"openai_gpt/","title":"Openai GPT","text":"<p>             Bases: <code>BaseServerObject</code></p> <p>Uses openai framework to interact with underlying gpt-* models. for reference visit : https://platform.openai.com/docs/models</p> Source code in <code>app/servers/gpt_server.py</code> <pre><code>class OpenaiGptExpert(BaseServerObject):\n    \"\"\"\n        Uses openai framework to interact with underlying gpt-* models.\n        for reference visit : https://platform.openai.com/docs/models \n    \"\"\"\n    def __init__(self,**kwargs):\n        \"\"\"\n        \"\"\"\n        self.config = kwargs\n        openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n        openai.api_key = os.environ.get('OPENAI_API_KEY')\n        self.openai_base = self.config.get('base_url')\n        self.model_id = self.config.get('model_id')\n        self.instance_name = self.model_id\n        self.config.update(dict(logs=[]))\n\n    def start_server(self):\n        instance_state = dict(\n            ip_address=\"ip_address\",\n            instance_name=\"openaigpt\"\n        )\n        self.config.update(instance_state)\n        self.config['logs'].append(f'Openai gpt server {self.model_id}')\n\n    def start_inference_endpoint(self, max_wait_time=120):\n        self.config['logs'].append(f'Openai gpt server inference endpoint {self.model_id}')\n\n    def stop_server(self):\n        pass\n\n    def check_servers_state(self):\n        return (True, 'running')\n\n    def get_response(self, message, stream=False):\n        messages = [{\"role\": \"user\", \"content\": message}]\n        if stream:\n            return self._generate_stream(messages)\n        else:\n            return self._generate(messages)\n\n    def _generate(self, messages):\n        try:\n            openai.api_base = self.openai_base\n            response = openai.ChatCompletion.create(\n                model=self.model_id,\n                messages=messages\n            )\n            answer = response['choices'][0]['message']['content']\n            return answer\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return \"Sorry, I couldn't process your request. Too many requests for me to handle!\"\n\n    def _generate_stream(self, messages):\n        try:\n            openai.api_base = self.openai_base\n            response = openai.ChatCompletion.create(\n                model=self.model_id,\n                messages=messages,\n                request_timeout=300,\n                stream=True\n            )\n            for chunk in response:\n                content = chunk['choices'][0]['delta'].get(\"content\", \"\")\n                yield content\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            yield \"Sorry, I couldn't process your request. Too many requests for me to handle!\"\n</code></pre>"},{"location":"openai_gpt/#app.servers.gpt_server.OpenaiGptExpert.__init__","title":"<code>__init__(**kwargs)</code>","text":"Source code in <code>app/servers/gpt_server.py</code> <pre><code>def __init__(self,**kwargs):\n    \"\"\"\n    \"\"\"\n    self.config = kwargs\n    openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n    openai.api_key = os.environ.get('OPENAI_API_KEY')\n    self.openai_base = self.config.get('base_url')\n    self.model_id = self.config.get('model_id')\n    self.instance_name = self.model_id\n    self.config.update(dict(logs=[]))\n</code></pre>"},{"location":"ssh_utils/","title":"SSH Utils","text":""},{"location":"ssh_utils/#app.utils.ssh_utils.capture_tmux_pane","title":"<code>capture_tmux_pane(ssh, tmux_session_name='vllm_server', verbose=True)</code>","text":"<p>Capture the content of the specified tmux pane.</p> <p>Parameters:</p> Name Type Description Default <code>ssh</code> <code>SSHClient</code> <p>The SSH client for the connection.</p> required <code>tmux_session_name</code> <code>str</code> <p>The name of the tmux session. Defaults to \"vllm_server\".</p> <code>'vllm_server'</code> <code>verbose</code> <code>bool</code> <p>If True, display verbose output. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing stdin, stdout, and stderr from the SSH command execution.</p> Source code in <code>app/utils/ssh_utils.py</code> <pre><code>def capture_tmux_pane(\n    ssh: paramiko.SSHClient,\n    tmux_session_name: str = \"vllm_server\",\n    verbose: bool =True\n):\n    \"\"\"\n    Capture the content of the specified tmux pane.\n\n    Args:\n        ssh (paramiko.SSHClient): The SSH client for the connection.\n        tmux_session_name (str, optional): The name of the tmux session. Defaults to \"vllm_server\".\n        verbose (bool, optional): If True, display verbose output. Defaults to True.\n\n    Returns:\n        tuple: A tuple containing stdin, stdout, and stderr from the SSH command execution.\n    \"\"\"\n    ssh_stdin, ssh_stdout, ssh_stderr = execute_ssh_command(ssh,\n        f\"tmux capture-pane -p -t {tmux_session_name}.0\", verbose)\n    return ssh_stdin, ssh_stdout, ssh_stderr\n</code></pre>"},{"location":"ssh_utils/#app.utils.ssh_utils.execute_ssh_command","title":"<code>execute_ssh_command(ssh, cmd, verbose=True)</code>","text":"<p>Runs the given command in paramiko ssh client </p> <p>Parameters:</p> Name Type Description Default <code>ssh</code> <code>SSHClient</code> <p>The SSH client for the connection.</p> required <code>cmd</code> <code>str</code> <p>Command that can run in tmux session terminal.</p> required Source code in <code>app/utils/ssh_utils.py</code> <pre><code>def execute_ssh_command(\n        ssh: paramiko.SSHClient,\n        cmd: str,\n        verbose: bool =True\n    ):\n    \"\"\"Runs the given command in paramiko ssh client \n\n    Args:\n        ssh (paramiko.SSHClient): The SSH client for the connection.\n        cmd (str): Command that can run in tmux session terminal.\n    \"\"\"\n    if verbose: print( f\"ssh : {cmd}\" )\n    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(cmd)\n    ssh_stdout_str = ssh_stdout.read().decode()\n    ssh_stderr_str = ssh_stderr.read().decode()\n    if verbose:\n        if ssh_stdout_str:\n            print(\"\\t stdout : \", ssh_stdout_str)\n        else:\n            print(\"\\t stderr : \", ssh_stderr_str)\n    return ssh_stdin, ssh_stdout_str, ssh_stderr_str\n</code></pre>"},{"location":"ssh_utils/#app.utils.ssh_utils.get_ssh_session","title":"<code>get_ssh_session(ip_address, timeout=15, key_path='app/keys/connection-key.pem')</code>","text":"<p>Get paramiko SSHClient  </p> <p>Parameters:</p> Name Type Description Default <code>ip_address</code> <code>str</code> <p>ip address</p> required <code>timeout</code> <code>int</code> <p>Defaults to 15.</p> <code>15</code> <code>key_path</code> <code>str) </code> <p>Path to the pem file required to connect to ec2 instance.</p> <code>'app/keys/connection-key.pem'</code> Source code in <code>app/utils/ssh_utils.py</code> <pre><code>def get_ssh_session(\n    ip_address: str, \n    timeout:int =15,\n    key_path:str =\"app/keys/connection-key.pem\"\n):\n    \"\"\"Get paramiko SSHClient  \n\n    Args:\n        ip_address (str): ip address\n        timeout (int, optional): Defaults to 15.\n        key_path (str) : Path to the pem file required to connect to ec2 instance. \n    \"\"\"\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    ssh.connect(ip_address, \n                username=\"ubuntu\", \n                key_filename=key_path,\n                timeout=timeout)\n    return ssh\n</code></pre>"},{"location":"ssh_utils/#app.utils.ssh_utils.is_running_vllm_server","title":"<code>is_running_vllm_server(ssh, tmux_session_name='vllm_server', verbose=True)</code>","text":"<p>Check if vllm server has started successfully.  </p> <p>Parameters:</p> Name Type Description Default <code>ssh</code> <code>SSHClient</code> <p>(paramiko.SSHClient):  The SSH client for the connection.</p> required <code>tmux_session_name</code> <code>str</code> <p>Defaults to \"vllm_server\".</p> <code>'vllm_server'</code> Source code in <code>app/utils/ssh_utils.py</code> <pre><code>def is_running_vllm_server(\n        ssh: paramiko.SSHClient,\n        tmux_session_name: str = \"vllm_server\",\n        verbose: bool =True\n    ):\n    \"\"\"Check if vllm server has started successfully.  \n\n    Args:\n        ssh: (paramiko.SSHClient):  The SSH client for the connection.\n        tmux_session_name (str, optional): Defaults to \"vllm_server\".\n    \"\"\"\n    ssh_stdin, ssh_stdout, ssh_stderr = capture_tmux_pane(ssh, tmux_session_name, verbose)\n    if \"Uvicorn running on http\" in ssh_stdout:\n        return True, \"running\"\n    elif \"python -m vllm\" in ssh_stdout or \\\n            \"llm_engine.py\" in ssh_stdout:\n        return False, \"loading\"\n    else:\n        return False, \"\"\n</code></pre>"},{"location":"ssh_utils/#app.utils.ssh_utils.start_vllm_server","title":"<code>start_vllm_server(ssh, model_id, ip_address, port=8000, conda_env_name='pytorch', tmux_session_name='vllm_server')</code>","text":"<p>Starts a VLLM Server for the provided Model ID.</p> <p>Parameters:</p> Name Type Description Default <code>ssh</code> <code>SSHClient</code> <p>The SSH client for the connection.</p> required <code>model_id</code> <code>str</code> <p>HF supported model ID. TODO: Support s3 checkpoint path downloading.</p> required <code>ip_address</code> <code>str</code> <p>The IP address for the EC2 instance.</p> required <code>port</code> <code>str</code> <p>VLLM runs a server that listens on this port. Ensure that this port is open for requests.</p> <code>8000</code> <code>conda_env_name</code> <code>str</code> <p>The specified environment should have VLLM installed.</p> <code>'pytorch'</code> <code>tmux_session_name</code> <code>str</code> <p>This is the name of the tmux session that runs the VLLM server.</p> <code>'vllm_server'</code> Source code in <code>app/utils/ssh_utils.py</code> <pre><code>def start_vllm_server(\n        ssh: paramiko.SSHClient,\n        model_id: str,\n        ip_address: str,\n        port: int =8000,\n        conda_env_name: str = \"pytorch\",\n        tmux_session_name: str = \"vllm_server\"\n    ):\n\n    \"\"\"Starts a VLLM Server for the provided Model ID.\n\n    Args:\n        ssh (paramiko.SSHClient):  The SSH client for the connection.\n        model_id (str): HF supported model ID. TODO: Support s3 checkpoint path downloading.\n        ip_address (str): The IP address for the EC2 instance.\n        port (str): VLLM runs a server that listens on this port. Ensure that this port is open for requests.\n        conda_env_name (str): The specified environment should have VLLM installed.\n        tmux_session_name (str): This is the name of the tmux session that runs the VLLM server.\n    \"\"\"\n\n    \"\"\"check if session is already created\"\"\"\n    _, ssh_stdout, _ = execute_ssh_command(ssh,\"tmux ls\")\n    if tmux_session_name not in ssh_stdout:\n        \"\"\"start a new tmux session\"\"\"\n        _,_,_ = execute_ssh_command(ssh,  \n                    f\"tmux new -d -s {tmux_session_name}\" )\n        time.sleep(5)\n\n        \"\"\"activate environment\"\"\"\n        _, _, _ = execute_ssh_command(ssh, \n                    f\"tmux send-keys -t {tmux_session_name}.0 'conda activate {conda_env_name}' ENTER\")\n        time.sleep(5)\n    else:\n        print(\"tmux session was found :\")\n        print(ssh_stdout)\n\n\n    \"\"\"confirm if session is created\"\"\"\n    _, ssh_stdout, _ = execute_ssh_command(ssh,\"tmux ls\")\n    assert tmux_session_name in ssh_stdout\n    print(ssh_stdout)\n\n\n    \"\"\"check if vllm server is already running\"\"\"\n    _,vllm_status = \\\n        is_running_vllm_server( ssh,  tmux_session_name)\n    if vllm_status not in ['running', 'loading']:\n        \"\"\"start vllm server\"\"\"\n        print( \"Vllm server starting... \" )\n        start_vllm_server = f\"python -m vllm.entrypoints.openai.api_server --model {model_id} --port {port}\"\n        _, _, _ = execute_ssh_command(ssh,\n            f\"tmux send-keys -t {tmux_session_name}.0 '{start_vllm_server}' ENTER\")\n        _,vllm_status = \\\n            is_running_vllm_server( ssh,  tmux_session_name)\n        print( f\"vllm status {vllm_status}\" )\n    else:\n        print( f\"vllm status {vllm_status}\" )\n</code></pre>"}]}